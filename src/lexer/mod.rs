use logos::Logos;

#[derive(Logos, Debug, Clone, PartialEq)]
#[logos(skip r"[ \t]+")]
pub enum Token {
    // Keywords
    #[token("let")]
    Let,

    #[token("if")]
    If,

    #[token("else")]
    Else,

    #[token("then")]
    Then,

    #[token("elif")]
    Elif,

    #[token("fi")]
    Fi,

    #[token("fn")]
    Fn,

    #[token("match")]
    Match,

    #[token("for")]
    For,

    #[token("in")]
    In,

    #[token("while")]
    While,

    #[token("do")]
    Do,

    #[token("done")]
    Done,

    #[token("until")]
    Until,

    #[token("function")]
    Function,

    #[token("case")]
    Case,

    #[token("esac")]
    Esac,

    // Operators and punctuation
    #[token("=")]
    Equals,

    #[token("==")]
    DoubleEquals,

    #[token("!=")]
    NotEquals,

    #[token(">=")]
    GreaterThanOrEqual,

    #[token("<=")]
    LessThanOrEqual,

    #[token(">")]
    GreaterThan,

    #[token("|||")]
    ParallelPipe,

    #[token("|")]
    Pipe,

    #[token("&&")]
    And,

    #[token("&")]
    Ampersand,

    #[token("||")]
    Or,

    #[token("!")]
    Bang,

    #[token("(")]
    LeftParen,

    #[token(")")]
    RightParen,

    #[token("{")]
    LeftBrace,

    #[token("}")]
    RightBrace,

    #[token("[")]
    LeftBracket,

    #[token("]")]
    RightBracket,

    #[token(";;")]
    DoubleSemicolon,

    #[token(";")]
    Semicolon,

    #[token(":")]
    Colon,

    #[token(",")]
    Comma,

    #[token(".")]
    Dot,

    #[token("->")]
    Arrow,

    #[token("=>")]
    FatArrow,

    // String literals
    #[regex(r#""([^"\\]|\\.)*""#, |lex| lex.slice().to_string())]
    String(String),

    #[regex(r"'([^'\\]|\\.)*'", |lex| lex.slice().to_string())]
    SingleQuotedString(String),

    // Numbers
    #[regex(r"-?[0-9]+", |lex| lex.slice().parse().ok())]
    Integer(i64),

    #[regex(r"-?[0-9]+\.[0-9]+", |lex| lex.slice().parse().ok())]
    Float(f64),

    // Glob patterns (*, ?, [...] wildcards in filename context)
    // Patterns with * or ? (e.g., *.rs, file?.txt, src/**/*.rs)
    #[regex(r"[a-zA-Z0-9_.\-/]*[*?][a-zA-Z0-9_.*?\-/\[\]]*", |lex| lex.slice().to_string())]
    // Bracket glob patterns (e.g., [abc].txt, file[0-9].txt)
    // Must have content after ] to distinguish from test builtin [ ]
    #[regex(r"[a-zA-Z0-9_.\-/]*\[[^\]]+\][a-zA-Z0-9_.*?\-/]+", |lex| lex.slice().to_string())]
    GlobPattern(String),

    // Identifiers and commands (dots allowed so filenames like README.md tokenize as one word)
    #[regex(r"[a-zA-Z_][a-zA-Z0-9_.\-]*", |lex| lex.slice().to_string())]
    Identifier(String),

    // Command substitution - needs custom parsing for nested cases
    #[regex(r"\$\(", parse_command_substitution)]
    CommandSubstitution(String),

    // Backtick command substitution
    #[regex(r"`", parse_backtick_substitution)]
    BacktickSubstitution(String),

    // Braced variables - must come before Special and Regular variables
    #[regex(r"\$\{[^}]+\}", |lex| lex.slice().to_string())]
    BracedVariable(String),

    // Special variables ($?, $!, $$, $#, $@, $*, $0-9, $-, $_)
    // Includes both single and special multi-char patterns
    #[regex(r"\$[?!$#@*\-_0-9]", |lex| lex.slice().to_string())]
    SpecialVariable(String),

    // Regular variables (at least 2 chars after $, or single letter)
    // This ensures $_ is matched as SpecialVariable, not Variable
    #[regex(r"\$[a-zA-Z][a-zA-Z0-9_]*", |lex| lex.slice().to_string())]
    Variable(String),

    // Standalone tilde (for tilde expansion: ~ expands to $HOME)
    #[token("~")]
    Tilde,

    // File paths and arguments
    #[regex(r"[.~/][^\s|;&(){}]+", |lex| lex.slice().to_string())]
    Path(String),

    // Flags
    // Bare dash (used in cd - for previous directory)
    #[token("-")]
    Dash,

    #[regex(r"-[a-zA-Z0-9]+", |lex| lex.slice().to_string())]
    ShortFlag(String),

    #[regex(r"--[a-zA-Z0-9][a-zA-Z0-9-]*", |lex| lex.slice().to_string())]
    LongFlag(String),

    // Plus flags (for unsetting shell options like +e, +u, +x)
    #[regex(r"\+[a-zA-Z0-9]+", |lex| lex.slice().to_string())]
    PlusFlag(String),

    // Redirects
    #[token(">>")]
    StdoutAppend,

    #[token("2>&1")]
    StderrToStdout,

    #[token("2>")]
    StderrRedirect,

    #[token("&>")]
    BothRedirect,

    // Here-document operators (must come before StdinRedirect for precedence)
    #[token("<<-")]
    HereDocStrip,

    #[token("<<")]
    HereDoc,

    #[token("<")]
    StdinRedirect,

    // Synthesized token: here-document body (not matched by lexer directly)
    HereDocBody(HereDocData),

    // Newline and EOF
    #[regex(r"\n")]
    Newline,

    #[token("\r\n")]
    CrLf,

    // Comments
    #[regex(r"#[^\n]*", logos::skip)]
    Comment,
}

/// Data for a synthesized here-document body token
#[derive(Debug, Clone, PartialEq)]
pub struct HereDocData {
    pub body: String,
    pub expand_vars: bool,
    pub strip_tabs: bool,
}

// Custom parser for $(...) that handles nesting
fn parse_command_substitution(lex: &mut logos::Lexer<Token>) -> Option<String> {
    let start = lex.span().start;
    let input = lex.source();
    let mut depth = 1; // We've consumed "$(" so one open paren
    let mut pos = lex.span().end;

    while pos < input.len() && depth > 0 {
        let ch = input.as_bytes()[pos] as char;
        match ch {
            '(' => depth += 1,
            ')' => depth -= 1,
            '\'' => {
                // Skip single-quoted string
                pos += 1;
                while pos < input.len() && input.as_bytes()[pos] as char != '\'' {
                    pos += 1;
                }
            }
            '"' => {
                // Skip double-quoted string
                pos += 1;
                while pos < input.len() {
                    let c = input.as_bytes()[pos] as char;
                    if c == '"' {
                        break;
                    }
                    if c == '\\' {
                        pos += 1; // skip escaped char
                    }
                    pos += 1;
                }
            }
            _ => {}
        }
        pos += 1;
    }

    if depth == 0 {
        // Extract the command including the $() delimiters
        let result = input[start..pos].to_string();
        // Update the lexer position
        lex.bump(pos - lex.span().end);
        Some(result)
    } else {
        None
    }
}

// Custom parser for backtick command substitution
fn parse_backtick_substitution(lex: &mut logos::Lexer<Token>) -> Option<String> {
    let start = lex.span().start;
    let input = lex.source();
    let mut pos = lex.span().end;

    // Find matching backtick
    while pos < input.len() {
        let ch = input.as_bytes()[pos] as char;
        if ch == '`' {
            pos += 1;
            let result = input[start..pos].to_string();
            lex.bump(pos - lex.span().end);
            return Some(result);
        } else if ch == '\\' && pos + 1 < input.len() {
            // Skip escaped character
            pos += 2;
        } else {
            pos += 1;
        }
    }

    None // Unclosed backtick
}

pub struct Lexer<'a> {
    inner: logos::Lexer<'a, Token>,
}

impl<'a> Lexer<'a> {
    pub fn new(input: &'a str) -> Self {
        Self {
            inner: Token::lexer(input),
        }
    }

    pub fn tokenize(input: &str) -> Result<Vec<Token>, LexerError> {
        let mut tokens = Vec::new();
        let mut lexer = Token::lexer(input);

        while let Some(token_result) = lexer.next() {
            match token_result {
                Ok(token) => tokens.push(token),
                Err(_) => {
                    return Err(LexerError::InvalidToken {
                        position: lexer.span().start,
                        text: lexer.slice().to_string(),
                    });
                }
            }
        }

        // Post-process: resolve here-documents
        let tokens = Self::resolve_heredocs(tokens, input);

        Ok(tokens)
    }

    /// Post-process token stream to resolve here-documents.
    fn resolve_heredocs(tokens: Vec<Token>, source: &str) -> Vec<Token> {
        let lines: Vec<&str> = source.lines().collect();
        let mut result: Vec<Token> = Vec::with_capacity(tokens.len());

        let mut i = 0;
        while i < tokens.len() {
            let is_heredoc = matches!(tokens[i], Token::HereDoc);
            let is_heredoc_strip = matches!(tokens[i], Token::HereDocStrip);

            if !is_heredoc && !is_heredoc_strip {
                result.push(tokens[i].clone());
                i += 1;
                continue;
            }

            let strip_tabs = is_heredoc_strip;
            i += 1; // skip << or <<-

            // Collect the delimiter word from subsequent tokens.
            let (delimiter, expand_vars) = if i < tokens.len() {
                match &tokens[i] {
                    Token::Identifier(s) => (s.clone(), true),
                    Token::SingleQuotedString(s) => {
                        let d = s.trim_matches('\'').to_string();
                        (d, false)
                    }
                    Token::String(s) => {
                        let d = s.trim_matches('"').to_string();
                        (d, false)
                    }
                    _ => {
                        if is_heredoc {
                            result.push(Token::HereDoc);
                        } else {
                            result.push(Token::HereDocStrip);
                        }
                        continue;
                    }
                }
            } else {
                if is_heredoc {
                    result.push(Token::HereDoc);
                } else {
                    result.push(Token::HereDocStrip);
                }
                continue;
            };
            i += 1; // skip delimiter token

            // Find which source line the << token is on by counting newlines
            let mut newline_count = 0;
            for t in &result {
                if matches!(t, Token::Newline | Token::CrLf) {
                    newline_count += 1;
                }
            }

            let body_start = newline_count + 1;

            let mut body_lines: Vec<String> = Vec::new();
            let mut body_end_line = body_start;
            let mut found_delimiter = false;

            for line_idx in body_start..lines.len() {
                let line = lines[line_idx];
                let trimmed = if strip_tabs {
                    line.trim_start_matches('\t')
                } else {
                    line
                };

                if trimmed.trim() == delimiter {
                    body_end_line = line_idx;
                    found_delimiter = true;
                    break;
                }

                let output_line = if strip_tabs {
                    line.trim_start_matches('\t').to_string()
                } else {
                    line.to_string()
                };
                body_lines.push(output_line);
            }

            if !found_delimiter {
                body_lines.clear();
            }

            let body = if body_lines.is_empty() {
                String::new()
            } else {
                body_lines.join("\n") + "\n"
            };

            result.push(Token::HereDocBody(HereDocData {
                body,
                expand_vars,
                strip_tabs,
            }));

            let lines_to_skip = if found_delimiter {
                body_end_line - newline_count
            } else {
                0
            };

            let mut newlines_skipped = 0;
            while i < tokens.len() && newlines_skipped < lines_to_skip {
                if matches!(tokens[i], Token::Newline | Token::CrLf) {
                    newlines_skipped += 1;
                }
                i += 1;
            }
            // Also skip any remaining tokens on the delimiter line
            // (e.g., the Identifier("EOF") token itself)
            while i < tokens.len() && !matches!(tokens[i], Token::Newline | Token::CrLf) {
                i += 1;
            }
        }

        result
    }
}

impl<'a> Iterator for Lexer<'a> {
    type Item = Result<Token, LexerError>;

    fn next(&mut self) -> Option<Self::Item> {
        self.inner.next().map(|result| {
            result.map_err(|_| LexerError::InvalidToken {
                position: self.inner.span().start,
                text: self.inner.slice().to_string(),
            })
        })
    }
}

#[derive(Debug, thiserror::Error)]
pub enum LexerError {
    #[error("Invalid token at position {position}: '{text}'")]
    InvalidToken { position: usize, text: String },
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_basic_command() {
        let tokens = Lexer::tokenize("ls -la /home").unwrap();
        assert_eq!(tokens.len(), 3);
        assert!(matches!(tokens[0], Token::Identifier(_)));
        assert!(matches!(tokens[1], Token::ShortFlag(_)));
        assert!(matches!(tokens[2], Token::Path(_)));
    }

    #[test]
    fn test_pipeline() {
        let tokens = Lexer::tokenize("ls | grep foo").unwrap();
        assert!(tokens.contains(&Token::Pipe));
    }

    #[test]
    fn test_variable() {
        let tokens = Lexer::tokenize("echo $HOME").unwrap();
        assert!(matches!(tokens[1], Token::Variable(_)));
    }

    #[test]
    fn test_string_interpolation() {
        let tokens = Lexer::tokenize(r#"echo "hello world""#).unwrap();
        assert!(matches!(tokens[1], Token::String(_)));
    }

    #[test]
    fn test_let_statement() {
        let tokens = Lexer::tokenize("let x = 42").unwrap();
        assert_eq!(tokens[0], Token::Let);
        assert!(matches!(tokens[1], Token::Identifier(_)));
        assert_eq!(tokens[2], Token::Equals);
        assert!(matches!(tokens[3], Token::Integer(42)));
    }

    #[test]
    fn test_function_definition() {
        let tokens = Lexer::tokenize("fn deploy(env: String) {}").unwrap();
        assert_eq!(tokens[0], Token::Fn);
        assert!(matches!(tokens[1], Token::Identifier(_)));
    }

    #[test]
    fn test_command_substitution_simple() {
        let tokens = Lexer::tokenize("echo $(pwd)").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::CommandSubstitution(cmd) = &tokens[1] {
            assert_eq!(cmd, "$(pwd)");
        } else {
            panic!("Expected CommandSubstitution token");
        }
    }

    #[test]
    fn test_command_substitution_nested() {
        let tokens = Lexer::tokenize("echo $(echo $(pwd))").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::CommandSubstitution(cmd) = &tokens[1] {
            assert_eq!(cmd, "$(echo $(pwd))");
        } else {
            panic!("Expected CommandSubstitution token");
        }
    }

    #[test]
    fn test_backtick_substitution() {
        let tokens = Lexer::tokenize("echo `pwd`").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::BacktickSubstitution(cmd) = &tokens[1] {
            assert_eq!(cmd, "`pwd`");
        } else {
            panic!("Expected BacktickSubstitution token");
        }
    }

    #[test]
    fn test_braced_variable_simple() {
        let tokens = Lexer::tokenize("echo ${VAR}").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::BracedVariable(var) = &tokens[1] {
            assert_eq!(var, "${VAR}");
        } else {
            panic!("Expected BracedVariable token, got {:?}", tokens[1]);
        }
    }

    #[test]
    fn test_braced_variable_use_default() {
        let tokens = Lexer::tokenize("echo ${VAR:-default}").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::BracedVariable(var) = &tokens[1] {
            assert_eq!(var, "${VAR:-default}");
        } else {
            panic!("Expected BracedVariable token, got {:?}", tokens[1]);
        }
    }

    #[test]
    fn test_braced_variable_assign_default() {
        let tokens = Lexer::tokenize("echo ${VAR:=default}").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::BracedVariable(var) = &tokens[1] {
            assert_eq!(var, "${VAR:=default}");
        } else {
            panic!("Expected BracedVariable token, got {:?}", tokens[1]);
        }
    }

    #[test]
    fn test_braced_variable_error_if_unset() {
        let tokens = Lexer::tokenize("echo ${VAR:?error}").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::BracedVariable(var) = &tokens[1] {
            assert_eq!(var, "${VAR:?error}");
        } else {
            panic!("Expected BracedVariable token, got {:?}", tokens[1]);
        }
    }

    #[test]
    fn test_braced_variable_prefix_removal() {
        let tokens = Lexer::tokenize("echo ${VAR#prefix}").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::BracedVariable(var) = &tokens[1] {
            assert_eq!(var, "${VAR#prefix}");
        } else {
            panic!("Expected BracedVariable token, got {:?}", tokens[1]);
        }
    }

    #[test]
    fn test_braced_variable_suffix_removal() {
        let tokens = Lexer::tokenize("echo ${VAR%suffix}").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::BracedVariable(var) = &tokens[1] {
            assert_eq!(var, "${VAR%suffix}");
        } else {
            panic!("Expected BracedVariable token, got {:?}", tokens[1]);
        }
    }

    #[test]
    fn test_special_variable_shell_pid() {
        let tokens = Lexer::tokenize("$$").unwrap();
        assert_eq!(tokens.len(), 1, "Should have 1 token for $$");
        if let Token::SpecialVariable(var) = &tokens[0] {
            assert_eq!(var, "$$", "Should be $$ token");
        } else {
            panic!("Expected SpecialVariable token for $$, got {:?}", tokens[0]);
        }
    }

    #[test]
    fn test_special_variable_last_bg_pid() {
        let tokens = Lexer::tokenize("$!").unwrap();
        assert_eq!(tokens.len(), 1);
        if let Token::SpecialVariable(var) = &tokens[0] {
            assert_eq!(var, "$!");
        } else {
            panic!("Expected SpecialVariable token for $!, got {:?}", tokens[0]);
        }
    }

    #[test]
    fn test_special_variable_option_flags() {
        let tokens = Lexer::tokenize("$-").unwrap();
        assert_eq!(tokens.len(), 1);
        if let Token::SpecialVariable(var) = &tokens[0] {
            assert_eq!(var, "$-");
        } else {
            panic!("Expected SpecialVariable token for $-, got {:?}", tokens[0]);
        }
    }

    #[test]
    fn test_special_variable_last_arg() {
        let tokens = Lexer::tokenize("$_").unwrap();
        assert_eq!(tokens.len(), 1);
        if let Token::SpecialVariable(var) = &tokens[0] {
            assert_eq!(var, "$_");
        } else {
            panic!("Expected SpecialVariable token for $_, got {:?}", tokens[0]);
        }
    }

    #[test]
    fn test_while_keyword() {
        let tokens = Lexer::tokenize("while").unwrap();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0], Token::While);
    }

    #[test]
    fn test_do_keyword() {
        let tokens = Lexer::tokenize("do").unwrap();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0], Token::Do);
    }

    #[test]
    fn test_done_keyword() {
        let tokens = Lexer::tokenize("done").unwrap();
        assert_eq!(tokens.len(), 1);
        assert_eq!(tokens[0], Token::Done);
    }

    #[test]
    fn test_filename_with_dot() {
        let tokens = Lexer::tokenize("cat README.md").unwrap();
        assert_eq!(tokens.len(), 2);
        assert!(matches!(tokens[0], Token::Identifier(_)));
        assert!(matches!(tokens[1], Token::Identifier(ref s) if s == "README.md"));
    }

    #[test]
    fn test_filename_multiple_dots() {
        let tokens = Lexer::tokenize("echo file.tar.gz").unwrap();
        assert_eq!(tokens.len(), 2);
        assert!(matches!(tokens[1], Token::Identifier(ref s) if s == "file.tar.gz"));
    }

    #[test]
    fn test_dot_alone_is_path() {
        let tokens = Lexer::tokenize("echo .").unwrap();
        assert_eq!(tokens.len(), 2);
        assert_eq!(tokens[1], Token::Dot);
    }

    #[test]
    fn test_dotdot_is_path() {
        let tokens = Lexer::tokenize("echo ..").unwrap();
        assert_eq!(tokens.len(), 2);
        assert!(matches!(tokens[1], Token::Path(ref s) if s == ".."));
    }

    #[test]
    fn test_dot_slash_path() {
        let tokens = Lexer::tokenize("./script.sh").unwrap();
        assert_eq!(tokens.len(), 1);
        assert!(matches!(tokens[0], Token::Path(ref s) if s == "./script.sh"));
    }

    #[test]
    fn test_tilde_standalone() {
        let tokens = Lexer::tokenize("echo ~").unwrap();
        assert_eq!(tokens.len(), 2);
        assert_eq!(tokens[1], Token::Tilde);
    }

    #[test]
    fn test_tilde_with_path() {
        let tokens = Lexer::tokenize("cd ~/Documents").unwrap();
        assert_eq!(tokens.len(), 2);
        assert!(matches!(tokens[1], Token::Path(ref s) if s == "~/Documents"));
    }

    #[test]
    fn test_tilde_user() {
        let tokens = Lexer::tokenize("echo ~root").unwrap();
        assert_eq!(tokens.len(), 2);
        assert!(matches!(tokens[1], Token::Path(ref s) if s == "~root"));
    }

    #[test]
    fn test_arithmetic_expansion_simple() {
        let tokens = Lexer::tokenize("echo $((1+2))").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::CommandSubstitution(cmd) = &tokens[1] {
            assert_eq!(cmd, "$((1+2))");
        } else {
            panic!("Expected CommandSubstitution token, got {:?}", tokens[1]);
        }
    }

    #[test]
    fn test_arithmetic_expansion_with_spaces() {
        let tokens = Lexer::tokenize("echo $((5 != 3))").unwrap();
        assert_eq!(tokens.len(), 2);
        if let Token::CommandSubstitution(cmd) = &tokens[1] {
            assert_eq!(cmd, "$((5 != 3))");
        } else {
            panic!("Expected CommandSubstitution token, got {:?}", tokens[1]);
        }
    }
}
