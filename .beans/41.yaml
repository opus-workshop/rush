id: '41'
title: 'AI-009: Performance benchmarking and optimization for AI agent workloads'
status: open
priority: 3
created_at: 2026-01-30T18:49:24.462116Z
updated_at: 2026-01-30T18:49:24.462116Z
description: "As a Rush developer, I need to measure and optimize performance for typical AI agent workloads so Rush is 10x faster than bash+jq.\n\n## Priority: P2 - MEDIUM\nPerformance is a key differentiator. We claim to be fast - let's prove it and optimize bottlenecks.\n\n## Acceptance Criteria\n- [ ] Benchmark suite for AI agent workflows\n- [ ] Comparison benchmarks vs bash+jq+curl\n- [ ] Identify and fix performance bottlenecks\n- [ ] Document performance characteristics\n- [ ] Achieve <5ms for git operations on typical repos\n- [ ] Achieve <1ms for JSON queries on typical data\n- [ ] Achieve 10x speedup vs bash for combined workflows\n- [ ] Continuous performance monitoring setup\n- [ ] cargo test passes\n- [ ] cargo build --release succeeds\n\n## Benchmark Scenarios\n\n### Scenario 1: Git Status Check Loop\n```bash\n# AI agents do this constantly while working\nfor i in {1..100}; do\n  git_status --json > /dev/null\ndone\n# Target: <500ms total (5ms per call)\n# Baseline (bash): ~2000ms (20ms per call)\n```\n\n### Scenario 2: Find + Filter + JSON\n```bash\n# Find all Rust files modified today\nfind --json src/ -name \"*.rs\" -mtime -1 | json_query '.[] | select(.size > 1000)'\n# Target: <10ms for 1000 files\n# Baseline (bash): ~100ms\n```\n\n### Scenario 3: Git Log + Analysis\n```bash\n# Get recent commits and extract authors\ngit_log --json -n 100 | json_get '.[].author' | sort | uniq\n# Target: <50ms\n# Baseline (bash + jq): ~200ms\n```\n\n### Scenario 4: HTTP + JSON Processing\n```bash\n# Fetch API data and extract field\nfetch --json https://api.github.com/repos/rust-lang/rust | json_get '.stargazers_count'\n# Target: Network-bound (no significant overhead)\n# Baseline (curl + jq): Same network time + ~5ms overhead\n```\n\n### Scenario 5: Complex Pipeline\n```bash\n# Realistic AI agent workflow\ngit_status --json | json_get '.unstaged[].path' | \\\n  while read file; do\n    grep --json \"TODO\" \"$file\"\n  done | json_query '.[] | {file, line: .line_number, text: .match}'\n# Target: <100ms for 50 files\n# Baseline (bash): ~500ms\n```\n\n## Technical Implementation\n\n### 1. Benchmark Infrastructure\n```rust\n// benches/ai_agent_workloads.rs\nuse criterion::{black_box, criterion_group, criterion_main, Criterion};\n\nfn bench_git_status_loop(c: &mut Criterion) {\n    c.bench_function(\"git_status_json_100x\", |b| {\n        b.iter(|| {\n            for _ in 0..100 {\n                // Call git_status --json\n                black_box(run_git_status());\n            }\n        });\n    });\n}\n\ncriterion_group!(benches, \n    bench_git_status_loop,\n    bench_find_filter_json,\n    bench_git_log_analysis,\n    // ...\n);\ncriterion_main!(benches);\n```\n\n### 2. Comparison Benchmarks\n```bash\n# benches/compare_bash.sh\n#!/usr/bin/env bash\nset -euo pipefail\n\necho \"Benchmarking Rush vs Bash...\"\n\n# Git status loop\necho \"Git status 100x:\"\ntime for i in {1..100}; do\n  ./target/release/rush -c \"git_status --json\" > /dev/null\ndone\n\ntime for i in {1..100}; do\n  bash -c \"git status --porcelain | jq ...\" > /dev/null\ndone\n\n# More comparisons...\n```\n\n### 3. Profiling Setup\n- Use flamegraph for CPU profiling\n- Use valgrind/heaptrack for memory profiling\n- Use perf for detailed analysis\n- Create profile targets in Cargo.toml\n\n### 4. Optimization Targets\n\n**Git Operations:**\n- Cache git2::Repository handle (don't reopen every call)\n- Lazy load git metadata\n- Parallel file status checking\n- Memoize branch/remote queries\n\n**JSON Operations:**\n- Streaming JSON parser for large files\n- JIT compilation of common queries (future)\n- Avoid unnecessary allocations\n- Use simd-json for parsing (evaluate)\n\n**File Operations:**\n- Parallel directory traversal\n- Memory-mapped file reading for large files\n- Optimize regex compilation (cache)\n\n**General:**\n- Reduce allocations in hot paths\n- Use `Cow<str>` where appropriate\n- Profile-guided optimization (PGO)\n- Link-time optimization (LTO) already enabled?\n\n## Performance Monitoring\n\n### 1. CI Integration\n- Run benchmarks on every PR\n- Fail if performance regresses >10%\n- Track performance over time\n- Use GitHub Actions + criterion\n\n### 2. Performance Dashboard\n- Generate HTML reports from criterion\n- Publish to GitHub Pages\n- Historical performance graphs\n- Compare across versions\n\n## Testing Strategy\n- Benchmark tests: ensure benchmarks run without errors\n- Regression tests: alert on slowdowns\n- Micro-benchmarks: for specific optimizations\n- Macro-benchmarks: for real-world workflows\n\n## Documentation\n- Create docs/PERFORMANCE.md:\n  - Benchmark results vs bash/zsh\n  - Performance characteristics of each builtin\n  - Optimization techniques used\n  - Tips for AI agents to maximize performance\n- Add to README:\n  - Key benchmark results\n  - \"10x faster\" claims with data\n- Inline comments explaining optimizations\n\n## Deliverables\n1. **Benchmark suite** (benches/ai_agent_workloads.rs)\n2. **Comparison scripts** (benches/compare_bash.sh)\n3. **Performance report** (PERFORMANCE.md)\n4. **Optimization implementations** (various files)\n5. **CI integration** (.github/workflows/bench.yml)\n6. **Performance dashboard** (GitHub Pages)\n\n## Dependencies\n- criterion = \"0.5\" (already in dev-dependencies?)\n- flamegraph (for profiling)\n- No additional runtime dependencies\n\n## Estimated Effort\n40-50k tokens:\n- Benchmark suite: 12k\n- Profiling & optimization: 15k\n- Comparison benchmarks: 8k\n- CI integration: 5k\n- Documentation: 10k"
labels:
- ai-agents
- benchmarking
- optimization
- p2
- performance
verify: test -f "docs/PERFORMANCE.md:\n"
